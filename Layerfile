#This is an example webapp.io configuration for NodeJS
FROM vm/ubuntu:18.04
# To note: Layerfiles create VMs, *not* containers!

RUN apt-get update && \
    apt-get install apt-transport-https ca-certificates curl software-properties-common && \
    curl -fsSL https://download.docker.com/linux/ubuntu/gpg | apt-key add - && \
    add-apt-repository "deb [arch=amd64] https://download.docker.com/linux/ubuntu bionic stable" && \
    apt-get update && \
    apt install docker-ce python3 python3-pip awscli && \
    curl -sL https://deb.nodesource.com/setup_14.x | bash && \
    apt install -y nodejs && \
    rm -f /etc/apt/sources.list.d/nodesource.list

# install docker compose (easily starts required docker containers)
RUN curl -L "https://github.com/docker/compose/releases/download/1.26.2/docker-compose-$(uname -s)-$(uname -m)" \
    -o /usr/local/bin/docker-compose

RUN npm install -g yarn pm2
RUN apt install unzip

COPY . .

MEMORY 4G

SECRET ENV SCHEMA_OBJ_URL
RUN mkdir db && curl "$SCHEMA_OBJ_URL" > db/schema.sql

RUN yarn
RUN yarn setup:ee
RUN NEXT_PUBLIC_INTERNAL_BACKEND_URL="https://$GIT_BRANCH.test-app.crusher.dev/server" NODE_OPTIONS=--max-old-space-size=8096 sh scripts/build/build-all.sh

RUN sudo chmod +x /usr/local/bin/docker-compose
RUN cp ./.env.oss .env

# Start mysql, redis and mongo
RUN REPEATABLE STANDALONE_APP_URL="https://$GIT_BRANCH.test-app.crusher.dev" docker-compose -f docker/ee/docker-compose.yml  up --build -d --force-recreate

RUN export LOGDNA_API_KEY=c7bdd500e3cfbfe457a2ec4168b8cfaa && \
    export DB_HOST=localhost && \
    export DP_USERNAME=root && \
    export DB_PASSWORD=password && \
    export DB_DATABASE=crusher && \
    export DB_PORT=3306 && \
    export MONGODB_CONNECTION_STRING=mongodb://localhost:27017/crusher && \
    export REDIS_HOST=localhost && \
    export REDIS_PORT=6379 && \
    export REDIS_PASSWORD= && \
    export STANDALONE_APP_URL=https://$GIT_BRANCH.test-app.crusher.dev && \
    export NEXT_PUBLIC_INTERNAL_BACKEND_URL=https://$GIT_BRANCH.test-app.crusher.dev/server && \
    export NEXT_PUBLIC_CRUSHER_MODE=enterprise

RUN node setup/dbMigration.js

RUN pm2 ecosystem.config.js --only "crusher-app,crusher-server,test-runner,video-processor"

EXPOSE WEBSITE http://localhost:3000

# To wait for server to starts

RUN node scripts/waitTillCrusherLoaded.js --url=https://$GIT_BRANCH.test-app.crusher.dev/server && curl --location --request POST 'https://backend.crusher.dev/projects/258/tests/actions/run' \
    --header 'Content-Type: application/x-www-form-urlencoded' \
    --cookie "token=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1c2VyX2lkIjoyODIsInRlYW1faWQiOjIxOSwiaWF0IjoxNjMzNDkyOTc5LCJleHAiOjE2NjUwMjg5Nzl9.fftN81qi0yJEDs9HnqiW4tElDMWp4dTHnAofnABmZuE" \
    --data-urlencode 'githubRepoName=crusherdev/crusher' \
    --data-urlencode "host=https://$GIT_BRANCH.test-app.crusher.dev" \
    --data-urlencode "githubCommitId=$GIT_COMMIT"
